{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 08. Exercise 02\n",
    "# Multiclass classification. One-hot encoding. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the file [`checker-submits.csv`](https://drive.google.com/file/d/14voc4fNJZiLEFaZyd8nEG-lQt5JjatYw/view?usp=sharing).\n",
    "2. Create a dataframe `df` with the columns: `uid`, `labname`, `numTrials`, `hour`, `dayofweek` where `hour` is extracted from the `timestamp` as well as the `dayofweek` (`0` is Monday, `6` is Sunday). We will try to predict the day of the week having data about which user made a commit for which lab at which hour and which try it was.\n",
    "3. Using `OneHotEncoder()` transform your categorical features, delete from the dataframe the initial columns.\n",
    "4. Use `StandardScaler()` and scale your continuous features.\n",
    "5. Save the dataframe as `dayofweek.csv`.\n",
    "6. Before trying out different algorithms, find out the accuracy of the naive algorithms ‚Äì the one that predicts everything as the most popular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º df —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏: uid, labname, numTrials, hour, dayofweek\n",
    "\n",
    "–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/checker_submits.csv', parse_dates=['timestamp'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–¥–µ–ª–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –ø–æ –∑–∞–¥–∞–Ω–∏—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "df = df.drop('timestamp', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –∏–º–µ—è –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–º, –∫–∞–∫–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–¥–µ–ª–∞–ª –∫–æ–º–º–∏—Ç, –¥–ª—è –∫–∞–∫–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏, –≤ –∫–∞–∫–æ–µ –≤—Ä–µ–º—è –∏ –∫–∞–∫–∞—è —ç—Ç–æ –±—ã–ª–∞ –ø–æ–ø—ã—Ç–∫–∞.\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É—è OneHotEncoder(), –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–≤–æ–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —É–¥–∞–ª–∏–º –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix=['uid', 'labname'], columns=['uid', 'labname'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è StandardScaler(), –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['numTrials', 'hour']] = scaler.fit_transform(df[['numTrials', 'hour']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/dayofweek.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, —É–∑–Ω–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–∏–≤–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ ‚Äî —Ç–æ–≥–æ, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∫–ª–∞—Å—Å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('dayofweek', axis=1)\n",
    "y = df['dayofweek']\n",
    "y_pred_naive = np.array([np.argmax(np.bincount(y))] * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –æ–±—ä–µ–∫—Ç –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–π –µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤–æ–∑–º–æ–∂–Ω—ã —á–µ—Ç—ã—Ä–µ —Å–∏—Ç—É–∞—Ü–∏–∏:\n",
    "\n",
    "–û–±—ä–µ–∫—Ç –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Å–∞ 1 –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–∞–∫ 1. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª –≤–µ—Ä–Ω–æ. –í–µ–ª–∏—á–∏–Ω–∞ TP (True positive) —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "–û–±—ä–µ–∫—Ç –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Å–∞ 0, –∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–∞–∫ 1. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª –Ω–µ –≤–µ—Ä–Ω–æ. –í–µ–ª–∏—á–∏–Ω–∞ FP (False positive) —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "–û–±—ä–µ–∫—Ç –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Å–∞ 0 –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–∞–∫ 0. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª –≤–µ—Ä–Ω–æ. –í–µ–ª–∏—á–∏–Ω–∞ TN (True negative) —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "–û–±—ä–µ–∫—Ç –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Å–∞ 1, –∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∫–∞–∫ 0. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª –Ω–µ –≤–µ—Ä–Ω–æ. –í–µ–ª–∏—á–∏–Ω–∞ FN (False negative) —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "Accuracy –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶=ùëáùëÉ+ùëáùëÅ / ùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train logistic regression, for the baseline model use `random_state=21`, `fit_intercept=False`. \n",
    "2. Calculate the accuracy.\n",
    "3. Write a function that draws the plot (`barh`) taking coefficients of any trained models, names of the features and the number of `top-n` most important features to display.\n",
    "4. Draw a plot (`barh`) for the baseline model with top-10 most important features (absolute value) for the trained model.\n",
    "5. Remember that it is a multiclass classification and `coef_` returns a matrix, to calculate importance for a feature you need to sum all the individual feature importances for all the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=21, fit_intercept=False)\n",
    "lr.fit(X, y)\n",
    "y_pred = lr.predict(X)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ (barh) –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è n –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö —Ñ–∏—á–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(coefs, features, n=10):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8)) # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "    coefs /= coefs.sum() # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –±–µ—Ä–µ–º —Å —É–¥–µ–ª—å–Ω—ã–º –≤–µ—Å–æ–º\n",
    "    indices = coefs.argsort()[::-1][:n] # –∏–Ω–¥–µ–∫—Å–∞–º–∏ –±–µ—Ä–µ–º —Ñ–∏—á–∏ (–∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é, –ø–æ—ç—Ç–æ–º—É —Å—Ä–µ–∑ —Å –∫–æ–Ω—Ü–∞, –ø–æ—Å–ª–µ–¥–Ω–∏–µ n\n",
    "    ax.barh(np.arange(n), coefs[indices], color='green')\n",
    "    ax.set_yticks(np.arange(n))\n",
    "    ax.set_yticklabels(features[indices])\n",
    "    ax.invert_yaxis() # —Å–¥–µ–ª–∞–µ–º, —á—Ç–æ–±—ã –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∏—á–∏ –±—ã–ª–∏ —Å–≤–µ—Ä—Ö—É\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(lr.coef_.mean(axis=0), X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a `SVC` model, for the baseline model use parameters `kernel='linear'`, `probability=True`, `random_state=21`. \n",
    "2. Try different kernels, calculate the accuracies.\n",
    "3. Draw a plot (`barh`) for the baseline model with top-10 most important features (absolute value) for the trained model for the linear kernel *\n",
    "\n",
    "*By default SVC uses ‚Äúone vs one‚Äù strategy of the classification, thus in `coef_` it returns a matrix. To calculate importance for a feature you need to use [OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) over the SVC and sum all the individual feature importances for all the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVS - –º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.\n",
    "\n",
    "http://datascientist.one/support-vector-machines/\n",
    "\n",
    "–û–±—É—á–∏–º –º–æ–¥–µ–ª—å SVC, –∏—Å–ø–æ–ª—å–∑—É–π—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–¥—Ä–∞ = '–ª–∏–Ω–µ–π–Ω—ã–π', –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å = –∏—Å—Ç–∏–Ω–∞, random_state = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear', probability=True, random_state=21)\n",
    "svc.fit(X, y)\n",
    "y_svc = svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—Ä–∏—Å—É–π–µ–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ (–∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–µ—Å–∞) –¥–ª—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —è–¥—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(svc.coef_.mean(axis=0), X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —è–¥—Ä–∞, –ø–æ—Å—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True, random_state=21)\n",
    "param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "gs = GridSearchCV(svc, param_grid, scoring='accuracy')\n",
    "gs.fit(X, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–õ—É—á—à–∏–º —è–¥—Ä–æ–º –æ–∫–∞–∑–∞–ª–æ—Å—å poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a `DecisionTreeClassifier` using for the baseline model `max_depth=4`, `random_state=21`. \n",
    "2. Try different values of `max_depth`, calculate the accuracies.\n",
    "3. Draw a plot (`barh`) for the baseline model with top-10 most important features (absolute value) for the trained model using the written function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏–º DecisionTreeClassifier, –∏—Å–ø–æ–ª—å–∑—É—è –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ max_depth=4, random_state=21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=4, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è max_depth, —Ä–∞—Å—Å—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=6, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=8, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=10, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=12, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=28, random_state=21)\n",
    "dtc.fit(X, y)\n",
    "y_dts = dtc.predict(X)\n",
    "accuracy_score(y, y_dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≥–ª—É–±–∏–Ω—ã —Ç–æ—á–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è. –ü—Ä–∏ –≥–ª—É–±–∏–Ω–µ 28 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è 1. –°–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–æ–π.\n",
    "\n",
    "–ù–∞—Ä–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏ (–ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é) –¥–ª—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–ø–∏—Å–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=21)\n",
    "param_grid = {'max_depth': [3, 5, 7, 10, 20, 28]}\n",
    "gs = GridSearchCV(dtc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(gs.best_estimator_.feature_importances_, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life forest is a set of trees. The same thing is with machine learning. Random forest is a set of individual decision trees (check the documentation for more details).\n",
    "\n",
    "1. Train a `RandomForestClassifier` using for the baseline model parameters `n_estimators=100`, `max_depth = 25`, `random_state=21`. \n",
    "2. Try different values of `max_depth` and `n_estimators`, calculate the accuracies.\n",
    "3. Draw a plot (`barh`) for the baseline model with top-10 most important features (absolute value) for the trained model using the written function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏–º RandomForestClassifier, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ n_estimators=100, max_depth = 25, random_state=21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=25,\n",
    "                             random_state=21)\n",
    "rfm.fit(X, y) # –æ–±—É—á–µ–Ω–∏–µ\n",
    "y_rfm = rfm.predict(X) # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "accuracy_score(y, y_rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è max_depth –∏ n_estimators, —Ä–∞—Å—Å—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=25,\n",
    "                             random_state=21)\n",
    "rfm.fit(X, y) # –æ–±—É—á–µ–Ω–∏–µ\n",
    "y_rfm = rfm.predict(X) # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "accuracy_score(y, y_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=20,\n",
    "                             random_state=21)\n",
    "rfm.fit(X, y) # –æ–±—É—á–µ–Ω–∏–µ\n",
    "y_rfm = rfm.predict(X) # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "accuracy_score(y, y_rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—Ä–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ (–ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é) –¥–ª—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–ø–∏—Å–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(random_state=21)\n",
    "param_grid = {'n_estimators': [100, 200, 300],\n",
    "              'max_depth': [3, 5, 7, 10, 20, 25, 30]}\n",
    "gs = GridSearchCV(rfm, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(gs.best_estimator_.feature_importances_, X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
