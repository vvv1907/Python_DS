{"cells":[{"cell_type":"markdown","metadata":{"id":"IW-ZM_6qI9vC"},"source":["# Day 08. Exercise 05\n","# Clustering"]},{"cell_type":"markdown","metadata":{"id":"KpC_dV4wI9vF"},"source":["## 0. Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doEusjhfI9vF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.cluster import KMeans\n","from sklearn import preprocessing\n","from sklearn import datasets\n","import sklearn.cluster as cluster\n","import sklearn.metrics as metrics\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import NearestNeighbors\n","import scipy.cluster.hierarchy as shc\n","from sklearn.cluster import AgglomerativeClustering"]},{"cell_type":"markdown","metadata":{"id":"9xbBFO-RI9vG"},"source":["## 1. Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"lqcSJn69I9vG"},"source":["1. Read the file [`regression.csv`](https://drive.google.com/file/d/1fzOPkuXoxLleOsvNVCT0m-LKxlid33ma/view?usp=sharing) to a dataframe.\n","2. Remove the `pageviews`, we will cluster the users only by the number of the commits and their average difference."]},{"cell_type":"markdown","metadata":{},"source":["Скачаем базу"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QFuN6PVI9vG"},"outputs":[],"source":["df = pd.read_csv('../data/regression.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Удалим просмотры страниц, мы будем группировать пользователей только по количеству коммитов и их средней разнице."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvq13XBkI9vH"},"outputs":[],"source":["df.drop('pageviews', axis=1, inplace=True)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"87wk7XpSI9vH"},"source":["## 2. KMeans"]},{"cell_type":"markdown","metadata":{"id":"cdySENr4I9vH"},"source":["1. Check the documentation about how this algorithm works.\n","2. Use this algorithm to create clusters, use `random_state=21` and `n_clusters=3`.\n","3. Visualize the data on a `scatter plot`.\n","4. Try different values of `n_clusters` and see how your plot will change.\n","5. Calculate the `silhouette_score` (check the docs about the metric)."]},{"cell_type":"markdown","metadata":{},"source":["Кластеризация: алгоритмы k-means и c-means теория: https://habr.com/ru/post/67078/ Метод выделения группы объектов, используя как меру расстояние. KMeans - наиболее простой, но не очень точный метод кластеризации в классической реализации. Он разбивает множество элементов векторного пространства на заранее известное число кластеров k. Действие алгоритма таково, что он стремится минимизировать среднеквадратичное отклонение на точках каждого кластера. Основная идея заключается в том, что на каждой итерации перевычисляется центр масс для каждого кластера, полученного на предыдущем шаге, затем векторы разбиваются на кластеры вновь в соответствии с тем, какой из новых центров оказался ближе по выбранной метрике. Алгоритм завершается, когда на какой-то итерации не происходит изменения кластеров. Нюансы:\n","\n","необходимо заранее знать количество кластеров\n","алгоритм чувствителен к выбору начальных центров код: https://habr.com/ru/company/otus/blog/666376/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPLC095QI9vH"},"outputs":[],"source":["kmeans = KMeans(n_clusters = 3, random_state = 21)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7p5_nGjI9vI"},"outputs":[],"source":["scaler = MinMaxScaler()\n","scale = scaler.fit_transform(df[['num_commits', 'AVG(diff)']])\n","df_scale = pd.DataFrame(scale, columns = ['num_commits', 'AVG(diff)']);\n","df_scale.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zq6NWe7pI9vI"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","ax=plt.subplot(2, 2, 1)\n","plt.title('2 clusters')\n","km=KMeans(n_clusters=2)\n","y_predicted = km.fit_predict(df[['num_commits', 'AVG(diff)']])\n","km.cluster_centers_\n","df['Clusters'] = km.labels_\n","sns.scatterplot(x='num_commits', y='AVG(diff)',hue = 'Clusters',  data=df,palette='viridis')\n","\n","\n","plt.subplot(2, 2, 2)\n","plt.title('3 clusters')\n","km=KMeans(n_clusters=3)\n","y_predicted = km.fit_predict(df[['num_commits', 'AVG(diff)']])\n","km.cluster_centers_\n","df['Clusters'] = km.labels_\n","sns.scatterplot(x='num_commits', y='AVG(diff)',hue = 'Clusters',  data=df,palette='viridis')\n","\n","plt.subplot(2, 2, 3)\n","plt.title('6 clusters')\n","km2=KMeans(n_clusters=6)\n","y_predicted2 = km2.fit_predict(df[['num_commits', 'AVG(diff)']])\n","km2.cluster_centers_\n","df['Clusters'] = km2.labels_\n","sns.scatterplot(x='num_commits', y='AVG(diff)',hue = 'Clusters',  data=df,palette='viridis')\n","\n","plt.subplot(2, 2, 4)\n","plt.title('8 clusters')\n","km=KMeans(n_clusters=8)\n","y_predicted = km.fit_predict(df[['num_commits', 'AVG(diff)']])\n","km.cluster_centers_\n","df['Clusters'] = km.labels_\n","sns.scatterplot(x='num_commits', y='AVG(diff)',hue = 'Clusters',  data=df,palette='viridis')"]},{"cell_type":"markdown","metadata":{},"source":["silhouette_score используется для определения дистанции между кластерами и выбора их оптимального количества.\n","https://machinelearningknowledge.ai/tutorial-for-k-means-clustering-in-python-sklearn/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYzah40gI9vI"},"outputs":[],"source":["for i in range(2,10):\n","    labels=cluster.KMeans(n_clusters=i,random_state=21).fit(df_scale).labels_\n","    print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"+str(metrics.silhouette_score(df_scale,labels,random_state=21)))"]},{"cell_type":"markdown","metadata":{},"source":["\n","Мы видим, что метод metrics.silhouette_score показывает нам, что оптимальное количество кластеров - 6. Однако, разброс между показателями 5-9 не такой уж большой. Соответственно, мы можем принять оптимальным количество кластеров от 5 до 9."]},{"cell_type":"markdown","metadata":{"id":"pAU7nYbDI9vI"},"source":["## 3. DBSCAN"]},{"cell_type":"markdown","metadata":{"id":"iERDnt7pI9vI"},"source":["1. Check the documentation about how this algorithm works.\n","2. Use this algorithm to create clusters with `eps=20` and `min_samples=2`.\n","3. Visualize the data on a `scatter plot`.\n","4. Try different values of `eps` and `min_samples` and see how your plot will change.\n","5. Calculate the `silhouette_score` (check the docs about the metric)."]},{"cell_type":"markdown","metadata":{},"source":["DBSCAN - Основанная на плотности пространственная кластеризация для приложений с шумами. Это алгоритм кластеризации, основанной на плотности — если дан набор точек в некотором пространстве, алгоритм группирует вместе точки, которые тесно расположены (точки со многими близкими соседями), помечая как выбросы точки, которые находятся одиноко в областях с малой плотностью (ближайшие соседи которых лежат далеко). DBSCAN является одним из наиболее часто используемых алгоритмов кластеризации, и наиболее часто упоминается в научной литературе.\n","\n","Алгоритм работает путем вычисления расстояния между каждой точкой и всеми другими точками. Затем мы помещаем точки в одну из трех категорий.\n","\n","Основная точка: точка, по крайней мере, сmin_samplesточки, расстояние которых относительно точки ниже порога, определенного эпсилоном.\n","\n","Граница: точка, которая не находится в непосредственной близости, по крайней мере, отmin_samplesточки, но достаточно близко к одной или нескольким основным точкам. Границы включены в кластер ближайшей базовой точки.\n","\n","Точка шума: точки, которые не достаточно близки к основным точкам, чтобы считаться пограничными точками. Шумовые точки игнорируются. То есть они не являются частью какого-либо кластера. https://machinelearningmastery.ru/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSGdOPx2I9vJ"},"outputs":[],"source":["x = df_scale.loc[:, ['num_commits', 'AVG(diff)']].values # достаем нужные значения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6iIjSRvI9vJ"},"outputs":[],"source":["print(x.shape)  #проверяем их количество"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMa8GNmzI9vJ"},"outputs":[],"source":["db = DBSCAN(eps = 20, min_samples = 2).fit_predict(df_scale)  \n","# применяем алгоритм определения количества кластеров, \n","#задаем мин количество значений в кластере (рекомендуется количество 2 * Data dimension, но у нас дано в сабдже)\n","#eps - расстояние до ближайшего соседа Nearest Neighbours\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise_ = list(labels).count(-1)\n","print('Estimated number of clusters: %d' % n_clusters_)\n","print('Estimated number of noise points: %d' % n_noise_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kD-nEXPEI9vJ"},"outputs":[],"source":["plt.scatter(x[:, 0], x[:,1], c = labels, cmap= \"plasma\") # plotting the clusters\n","plt.xlabel(\"num_commits\") # X-axis label\n","plt.ylabel(\"AVG(diff)\") # Y-axis label\n","plt.show() # showing the plot"]},{"cell_type":"markdown","metadata":{},"source":["Calculate the average distance between each point in the data set and its 20 nearest neighbors (my selected MinPts value). https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d125rzOgI9vJ"},"outputs":[],"source":["neighbors = NearestNeighbors(n_neighbors=5)\n","# n_neighbors = 5 as kneighbors function returns distance of point to itself (i.e. first column will be zeros) \n","nbrs = NearestNeighbors(n_neighbors=5).fit(df_scale)\n","neighbors_fit = neighbors.fit(df_scale)\n","distances, indices = neighbors_fit.kneighbors(df_scale)\n","#(kNN) distances - average distance of every data point to its k-nearest neighbors\n","distances = np.sort(distances, axis=0)\n","# sort the neighbor distances (lengths to points) in ascending order\n","# axis = 0 represents sort along first axis i.e. sort along row\n","distances = distances[:,4]\n","plt.ylabel(\"k-NN distance\")\n","plt.xlabel(\"Sorted observations\")\n","plt.plot(distances)"]},{"cell_type":"markdown","metadata":{},"source":["Пробуем с разными значениями"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGs4Lo7OI9vJ"},"outputs":[],"source":["clusters = DBSCAN(eps = 22, min_samples = 4).fit(df_scale)\n","clusters.labels_\n","# применяем алгоритм определения количества кластеров, \n","#задаем мин количество значений в кластере (рекомендуется количество 2 * Data dimension, но у нас дано в сабдже)\n","#eps - расстояние до ближайшего соседа Nearest Neighbours\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise_ = list(labels).count(-1)\n","\n","print('Estimated number of clusters: %d' % n_clusters_)\n","print('Estimated number of noise points: %d' % n_noise_)\n","plt.scatter(x[:, 0], x[:,1], c = labels, cmap= \"plasma\") # plotting the clusters\n","plt.xlabel(\"num_commits\") # X-axis label\n","plt.ylabel(\"AVG(diff)\") # Y-axis label\n","plt.show() # showing the plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIHo7ZHXI9vK"},"outputs":[],"source":["print (\"Silhouette score for 9 clusters is \"+str(metrics.silhouette_score(df_scale,labels)))"]},{"cell_type":"markdown","metadata":{"id":"ChN80omRI9vK"},"source":["## 4. Hierarchical (AgglomerativeClustering)"]},{"cell_type":"markdown","metadata":{"id":"CZYmTp-EI9vK"},"source":["1. Use this algorithm to create clusters with `n_clusters=5`.\n","2. Visualize the data on a `scatter plot`.\n","3. Try different values of `n_clusters` and see how your plot will change.\n","4. Calculate the `silhouette_score`.\n","5. Visualize the `dendrogram`."]},{"cell_type":"markdown","metadata":{},"source":["Иерархическая кластеризация — это неконтролируемый метод обучения для кластеризации точек данных. Алгоритм строит кластеры, измеряя различия между данными. Неконтролируемое обучение означает, что модель не нужно обучать, и нам не нужна «целевая» переменная. Этот метод можно использовать для любых данных, чтобы визуализировать и интерпретировать взаимосвязь между отдельными точками данных. https://www.w3schools.com/python/python_ml_hierarchial_clustering.asp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VomrjURdI9vK"},"outputs":[],"source":["data = df_scale"]},{"cell_type":"markdown","metadata":{},"source":["Используем алгоритм для создания кластеров с n_clusters=5.\n","\n","Визуализируем данные на «диаграмме рассеивания».\n","\n","Попробуем разные значения n_clusters и посмотрим, как изменится график."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUJHbbjpI9vK"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","ax=plt.subplot(2, 2, 1)\n","hierarchical_cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n","labels = hierarchical_cluster.fit_predict(data)\n","hierarchical_cluster.labels_\n","data_labels = hierarchical_cluster.labels_\n","sns.scatterplot(x='num_commits', \n","                y='AVG(diff)', \n","                data=data, \n","                palette='viridis',\n","                hue=data_labels).set_title('5 clusters')\n","\n","ax=plt.subplot(2, 2, 2)\n","hierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n","labels = hierarchical_cluster.fit_predict(data)\n","hierarchical_cluster.labels_\n","data_labels = hierarchical_cluster.labels_\n","sns.scatterplot(x='num_commits', \n","                y='AVG(diff)', \n","                data=data, \n","                palette='viridis',\n","                hue=data_labels).set_title('3 clusters')\n","\n","ax=plt.subplot(2, 2, 3)\n","hierarchical_cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n","labels = hierarchical_cluster.fit_predict(data)\n","hierarchical_cluster.labels_\n","data_labels = hierarchical_cluster.labels_\n","sns.scatterplot(x='num_commits', \n","                y='AVG(diff)', \n","                data=data, \n","                palette='viridis',\n","                hue=data_labels).set_title('6 clusters')\n","\n","ax=plt.subplot(2, 2, 4)\n","hierarchical_cluster = AgglomerativeClustering(n_clusters=8, affinity='euclidean', linkage='ward')\n","labels = hierarchical_cluster.fit_predict(data)\n","hierarchical_cluster.labels_\n","data_labels = hierarchical_cluster.labels_\n","sns.scatterplot(x='num_commits', \n","                y='AVG(diff)', \n","                data=data, \n","                palette='viridis',\n","                hue=data_labels).set_title('8 clusters')"]},{"cell_type":"markdown","metadata":{},"source":["Рассчитаем silhouette_score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9yZInN5I9vK"},"outputs":[],"source":["for i in range(2,10):\n","    labels=cluster.KMeans(n_clusters=i,random_state=21).fit(data).labels_\n","    print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"+str(metrics.silhouette_score(data,labels,random_state=21)))"]},{"cell_type":"markdown","metadata":{},"source":["Визуализируем «дендрограмму»."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WOqDz-_I9vL"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.title(\"Dendrogram\")\n","selected_data = data\n","clusters = shc.linkage(data, \n","            method='ward', \n","            metric=\"euclidean\")\n","shc.dendrogram(Z=clusters)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"oZJCHSbYI9vL"},"source":["## 5. Function"]},{"cell_type":"markdown","metadata":{"id":"Dk8GswaAI9vL"},"source":["Write a function that:\n","1. Takes as arguments: model class of clustering, its parameters, the name of the parameter for optimization, the range of the parameter values to try.\n","2. Tries different values from the given parameter range and calculates the `silhouette_score` for each value from the range.\n","3. Finds out the best value for the parameter in the range.\n","4. Returns two subplots:\n","\n","\n","- - the first shows how the `silhouette_score` changes depending on the value of the parameter,\n","- - the second visualizes the data on a `scatter plot` using the clustering model with the best value of the parameter."]},{"cell_type":"markdown","metadata":{},"source":["Напишим функцию, которая:\n","\n","Принимает в качестве аргументов: класс модели кластеризации, ее параметры, имя параметра для оптимизации, диапазон значений параметров, которые нужно попробовать.\n","\n","Пробует разные значения из заданного диапазона параметров и вычисляет silhouette_score для каждого значения из диапазона.\n","\n","Находит лучшее значение параметра в диапазоне.\n","\n","Возвращает два подграфика:\n","\n","-- первый показывает, как меняется silhouette_score в зависимости от значения параметра,\n","\n","второй визуализирует данные на графике рассеяния, используя модель кластеризации с лучшим значением параметра."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a77coOzAI9vL"},"outputs":[],"source":["from yellowbrick.cluster import silhouette_visualizer\n","from yellowbrick.datasets import load_nfl\n","\n","def my_function(clustering, clust_args, name, param_range):\n","    df_test = pd.read_csv('../data/regression.csv')\n","    df_test.drop('pageviews', axis=1, inplace=True)\n","    scaler = MinMaxScaler()\n","    scale = scaler.fit_transform(df[['num_commits', 'AVG(diff)']])\n","    df_test = pd.DataFrame(scale, columns = ['num_commits', 'AVG(diff)']);\n","\n","    if clustering == 'kmeans':\n","        kmeans = KMeans(n_clusters = clusters, random_state = 21)\n","        labels=cluster.KMeans(n_clusters=param_range[0],random_state=21).fit(df_test).labels_\n","        best = metrics.silhouette_score(df_test, labels, metric='euclidean')\n","        best_num = param_range[0]\n","        for i in range(param_range[0],param_range[1]):\n","            labels=cluster.KMeans(n_clusters=i,random_state=21).fit(df_test).labels_\n","            print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"+str(metrics.silhouette_score(df_test,labels,random_state=21)))\n","            new = metrics.silhouette_score(df_test,labels,random_state=21)\n","            if  new > best:\n","                best = metrics.silhouette_score(df_test,labels,random_state=21)\n","                best_num = i\n","        print('The best value is', best, 'for', best_num, 'clusters')\n","        plt.figure(figsize=(8,8))\n","        ax=plt.subplot(1, 2, 1)    \n","        silhouette_visualizer(KMeans(n_clusters=6, random_state=21), df_test, colors='yellowbrick')\n","        \n","        ax=plt.subplot(1, 2, 2)   \n","        plt.title('Best Kmeans clustering')\n","        km=KMeans(n_clusters=2)\n","        y_predicted = km.fit_predict(df[['num_commits', 'AVG(diff)']])\n","        km.cluster_centers_\n","        df['Clusters'] = km.labels_\n","        sns.scatterplot(x='num_commits', y='AVG(diff)',hue = 'Clusters',  data=df,palette='viridis')\n","    \n","\n","    \n","if __name__ == '__main__':\n","     my_function('kmeans', [2], clusters, [2,8])"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"05_clustering.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
