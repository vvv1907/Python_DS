{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 00\n",
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py:82\u001b[0m\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=79'>80</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=80'>81</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=81'>82</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=82'>83</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=84'>85</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=85'>86</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=86'>87</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=127'>128</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/__init__.py?line=128'>129</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:17\u001b[0m\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m---> <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=18'>19</a>\u001b[0m     _DEFAULT_TAGS,\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/base.py?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m check_X_y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py:22\u001b[0m\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m issparse\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmurmurhash\u001b[39;00m \u001b[39mimport\u001b[39;00m murmurhash3_32\n\u001b[1;32m     <a href='file:///Users/hcolumbu/Library/Python/3.8/lib/python/site-packages/sklearn/utils/__init__.py?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.sparse'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the file `dayofweek.csv` that you used in the previous day to a dataframe.\n",
    "2. Using `train_test_split` with parameters `test_size=0.2`, `random_state=21` get `X_train`, `y_train`, `X_test`, `y_test`. Use the additional parameter `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dayofweek.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('dayofweek', axis=1)\n",
    "y = df['dayofweek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logreg regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Default regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a baseline model with the only parameters `random_state=21`, `fit_intercept=False`.\n",
    "2. Use stratified K-fold cross-validation with `10` splits to evaluate the accuracy of the model\n",
    "\n",
    "\n",
    "The result of the code where you trained and evaluated the baseline model should be exactly like this (use `%%time` to get the info about how long it took to run the cell):\n",
    "\n",
    "```\n",
    "train -  0.62902   |   valid -  0.59259\n",
    "train -  0.64633   |   valid -  0.62963\n",
    "train -  0.63479   |   valid -  0.56296\n",
    "train -  0.65622   |   valid -  0.61481\n",
    "train -  0.63397   |   valid -  0.57778\n",
    "train -  0.64056   |   valid -  0.59259\n",
    "train -  0.64138   |   valid -  0.65926\n",
    "train -  0.65952   |   valid -  0.56296\n",
    "train -  0.64333   |   valid -  0.59701\n",
    "train -  0.63674   |   valid -  0.62687\n",
    "Average accuracy on crossval is 0.60165\n",
    "Std is 0.02943\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(fit_intercept=False, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(estimator, X, y, n_splits=10):\n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    cv = StratifiedKFold(n_splits=n_splits)\n",
    "    for train, valid in cv.split(X, y):\n",
    "        estimator.fit(X.iloc[train], y.iloc[train])\n",
    "        y_train_pred = estimator.predict(X.iloc[train])\n",
    "        y_valid_pred = estimator.predict(X.iloc[valid])\n",
    "        train_scores.append(accuracy_score(y.iloc[train], y_train_pred))\n",
    "        valid_scores.append(accuracy_score(y.iloc[valid], y_valid_pred))\n",
    "    for i in range(len(train_scores)):\n",
    "        print(f'train -  {train_scores[i]:.5f}   |   valid -  {valid_scores[i]:.5f}')\n",
    "    print(f'Average accuracy on crossval is {np.mean(valid_scores):.5f}')\n",
    "    print(f'Std is {np.std(valid_scores):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crossval(lr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Optimizing regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the cells below try different values of penalty: `none`, `l1`, `l2` â€“ you can change the values of solver too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "              {'solver': ['liblinear', 'saga'], 'penalty': ['l1', 'l2']},\n",
    "              {'solver': ['saga'], 'penalty': ['l1', 'l2', 'none']},\n",
    "              {'solver': ['newton-cg', 'lbfgs', 'sag'], 'penalty': ['l2', 'none']}]\n",
    "\n",
    "gs = GridSearchCV(lr, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Default regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a baseline model with the only parameters `probability=True`, `kernel='linear'`, `random_state=21`.\n",
    "2. Use stratified K-fold cross-validation with `10` splits to evaluate the accuracy of the model.\n",
    "3. The format of the result of the code where you trained and evaluated the baseline model should be similar to what you have got for the logreg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear', probability=True, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crossval(svc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Optimizing regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the cells below try different values of the parameter `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [15, 40, 60, 100, 150]}\n",
    "gs = GridSearchCV(svc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Default regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a baseline model with the only parameter `max_depth=10` and `random_state=21`.\n",
    "2. Use stratified K-fold cross-validation with `10` splits to evaluate the accuracy of the model.\n",
    "3. The format of the result of the code where you trained and evaluated the baseline model should be similar to what you have got for the logreg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=10, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crossval(dtc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Optimizing regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the cells below try different values of the parameter `max_depth`.\n",
    "2. As a bonus, play with other regularization parameters trying to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'splitter': ['best', 'random'],\n",
    "              'max_depth': np.arange(2, 25)}\n",
    "gs = GridSearchCV(dtc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Default regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a baseline model with the only parameters `n_estimators=50`, `max_depth=14`, `random_state=21`.\n",
    "2. Use stratified K-fold cross-validation with `10` splits to evaluate the accuracy of the model.\n",
    "3. The format of the result of the code where you trained and evaluated the baseline model should be similar to what you have got for the logreg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=14, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crossval(rfc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Optimizing regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the new cells try different values of the parameters `max_depth` and `n_estimators`.\n",
    "2. As a bonus, play with other regularization parameters trying to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': np.arange(15, 30),\n",
    "              'n_estimators': np.arange(50, 150)}\n",
    "gs = GridSearchCV(rfc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the best model and use it to make predictions for the test dataset.\n",
    "2. Calculate the final accuracy.\n",
    "3. Analyze: for which weekday your model makes the most errors (in % of the total number of samples of that class in your test dataset).\n",
    "4. Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(gs.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gs.best_estimator_, '../data/model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
